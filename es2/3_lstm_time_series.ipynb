{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys; sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.data.santa_fe import load_santa_fe\n",
    "from lib.utils.time_series import create_time_series_inputs_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9dbfc4",
   "metadata": {},
   "source": [
    "# 3. Time-Series Prediction with a Long Short-Term Memory Network\n",
    "\n",
    "Long Short Term Memory networks, usually just called \"LSTMs\", are a special kind of RNN, capable of learning long-term dependencies ([Hochreiter and Schmidhuber, 1997](https://www.nichenjie.com/mdres/posts/2018/lstm/2604.pdf)). Wheras the MLP-based recurrent model we discussed in the previous notebook takes in an entire (fixed-size) time series as input, an LSTM takes in the time series data points *one at a time*. For each data point, the LSTM cell produces two outputs: a hidden state $h_t$ and a cell state $c_t$. The hidden state and the cell state correspond to short-term and long-term memory, respectively. Apart from the time-series data point $x_t$, an LSTM cell takes two other inputs: the previous hidden state $h_{t-1}$ and the previous cell state $c_{t-1}$.\n",
    "\n",
    "The hidden state $h_t$ produced by the LSTM can be interpreted as a vector-representation of the predicted data point at the next time instant. In our implementation below, we will project $h_t$ onto a single scalar, which will then be the predicted data point itself.\n",
    "\n",
    "As you can see in the figure below, the LSTM cell modifies the previous cell state and hidden state. This happens via a *gating* mechanism. Based on the previous hidden state $h_{t-1}$ and the current data point $x_t$, the LSTM cell computes three *gating vectors* (the outputs of each $\\sigma$ operation) that contain elements between 0 and 1. When element-wise multiplying such a gating vector with another vector, each element of the gating vector acts as a little *gate* that allows data of the other vector to (partially) pass through or not. A zero blocks the data, a one lets the data through.\n",
    "\n",
    "For example, the *cell gate* is the gate produced by the $\\sigma$ operation at the left-hand side of the figure. It chooses which elements of the previous cell state $c_{t-1}$ will be kept and which will be removed. The *input gate* (second from the left) chooses which information will be added to the new cell state, while the *output gate* (right-hand side $\\sigma$) chooses which information of the new cell state $c_t$ (after applying $\\tanh$) to use for $h_t$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/93/LSTM_Cell.svg\" style=\"max-width: 500px;\"/>\n",
    "\n",
    "Note that **the same cell is used at each time instant**, but always with **new inputs** $h_{t-1}$, $c_{t-1}$ and $x_t$.\n",
    "\n",
    "The output of every gate is produced by passing $x_t$ and $h_{t-1}$ each through their own fully-connected layer, adding the results and applying a sigmoid operation. For producing the result of the $\\tanh$ branch in the middle of the figure, $x_t$ and $h_{t-1}$ are also each passed through their own fully-connected layer. The results are again added together but passed through a $\\tanh$ operation instead of a sigmoid.\n",
    "\n",
    "As such, the LSTM cell contains **8 fully-connected layers** with learnable parameters. These parameters can be trained via a gradient descent-based optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b7c72",
   "metadata": {},
   "source": [
    "## 3.1 LSTM-based Model for Time-Series Prediction\n",
    "\n",
    "We have implemented an LSTM-based regression model below. The model employs [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) to process a sequence of inputs. As you can see in the `forward()` method, the resulting hidden state vector is passed through a fully-connected layer, which transforms it into a prediction for the next time instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size=1, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): The number of features to use for the hidden state.\n",
    "            input_size (int): The number of dimensions a measurement has at a single time step.\n",
    "            num_layers (int): Number of recurrent layers. E.g., setting `num_layers=2` would\n",
    "                mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM\n",
    "                taking in the hidden states of the first LSTM and computing the final results.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize the LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        # Initialize the fully-connected layer\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.input_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of shape `(N, L, H_in)`, `(N, L)` or `(L,)` with `N`\n",
    "                the batch size, `L` the sequence length and `H_in` the input size. If\n",
    "                the dimensions of `L` and/or `H_in` are not given, they are assumend to\n",
    "                be 1.\n",
    "        \"\"\"\n",
    "        orig_ndim = x.ndim\n",
    "\n",
    "        if orig_ndim == 1:\n",
    "            # Assume batch size and input size are 1\n",
    "            x = x[None, ..., None]\n",
    "        if orig_ndim == 2:\n",
    "            # Assume input size is 1\n",
    "            x = x[..., None]\n",
    "\n",
    "        N, L, H_in = x.shape\n",
    "\n",
    "        assert self.input_size == H_in\n",
    "\n",
    "        # Reshape to (L, N, H_in) for LSTM input\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # `output` contains hidden state from last layer for each sequence element\n",
    "        # `h_n` contains the hidden state from all layers for the last sequence element\n",
    "        # `c_n` contains the cell state from all layers for the last sequence element\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Select the last hidden state from the last element...\n",
    "        h_last = h_n[-1]  # (Note: same as output[-1])\n",
    "\n",
    "        # ...and pass it through a fully-connected layer\n",
    "        out = self.linear(h_last)\n",
    "\n",
    "        if orig_ndim == 1:\n",
    "            # Remove batch dimension\n",
    "            out = out[0, :]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cfd0f",
   "metadata": {},
   "source": [
    "The number of features in the hidden state can be controlled with the `hidden_size` argument of the constructor. For example, one can create an LSTM-based model with a hidden state vector of length 128 as follows:\n",
    "\n",
    "```python\n",
    "# Create the LSTM model\n",
    "lstm = LSTMRegressor(hidden_size=128)\n",
    "```\n",
    "\n",
    "Once you have created the model, you can use it in a similar way as the MLP from the [previous notebook](./2_nn_time_series.ipynb):\n",
    "\n",
    "```python\n",
    "# Predict the next time series value\n",
    "y_next = lstm(time_series)\n",
    "```\n",
    "\n",
    "Where `time_series` is a tensor of shape $(N, L)$ with $N$ the batch size and $L$ the length of a single time series sequence. Then, `y_next` will be a tensor of shape $(N, 1)$ containing the next predicted value for each sequence. Just like with the MLP, we can compare these predictions with the ground-truth values and compute (and backpropagate) the loss.\n",
    "\n",
    "## 3.2 Making Time-Series Predictions\n",
    "\n",
    "In the previous notebook, you have implemented the function `run_recurrent_model()` to produce time-series predictions for an MLP-based recurrent model. One limitation of such a model is that the sequence length is *fixed*, i.e., it is equal to the number of input neurons of the MLP. This is an important difference with an LSTM-based model. An LSTM takes in a data point at a single time instant, the previous hidden state and the previous cell state and produces the next hidden state and the next cell state. There is no restriction on the length of the time-series sequence.\n",
    "\n",
    "Paste your `run_recurrent_model()` implementation of the previous notebook below and modify it so that the length of the time-series grows as the model predicts more data points (instead of dropping the oldest data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad24f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your modified run_recurrent_model() implementation here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0e1d5",
   "metadata": {},
   "source": [
    "## 3.3 Exercise\n",
    "\n",
    "Reuse your training code from the [previous notebook](./2_nn_time_series.ipynb) to model the Santa Fe dataset with the `LSTMRegressor` class.\n",
    "\n",
    "- Train the LSTM-based model and explain the design process. Discuss how the model looks, the parameters that you tune,... What is the effect of changing the lag value for the LSTM network?\n",
    "- Afterwards, try to predict the validation set. Use your newly implemented `run_recurrent_model()` function for this.\n",
    "- Compare results of the recurrent neural network of the previous notebook with the LSTM. Which model do you prefer and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1a4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d525086e",
   "metadata": {},
   "source": [
    "# 4 Report\n",
    "\n",
    "Write a report of maximum 4 pages (including text and figures) to discuss the exercises in sections 1, 2 and 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('anndl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f299b0a30a695930222e2b85814db2feaa48d467a39d54db5a092acf936d7d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
