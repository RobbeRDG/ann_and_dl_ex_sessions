{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6c820-6c4e-4510-b463-f67774086f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from random import randint\n",
    "import sys; sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import tensor, nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib.metrics.accuracy import get_accuracy\n",
    "from lib.models.hlim import HardLimit\n",
    "from lib.models.perceptron import create_perceptron\n",
    "from lib.widgets.mlp_response_view import MLPResponseView\n",
    "from lib.widgets.mlp_arch_view import MLPArchView\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f9265-40fa-4ec8-92b5-c1503a4c0247",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. The perceptron\n",
    "\n",
    "The perceptron is the simplest one-layer network. It consists of $R$ inputs and 1 output. Each of the $R$ inputs is multiplied by a *weight*. The outputs of these multiplications are summed and a *bias* is added to the result. Finally, the resulting number will be passed through the *hard-limit* activation function. This function returns $1$ whenever its input is positive and $0$ when the input is negative. The perceptron is used as a simple classification tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef536ea8-b035-4769-b293-f401d45c21aa",
   "metadata": {},
   "source": [
    "You can use the function `get_perceptron()` to create a perceptron with a certain number of input neurons and a single output neuron. With the `MLPArchView` and `MLPResponseView` classes, you can visualize the architecture and the response of the perceptron.\n",
    "\n",
    "The response plot shows a heatmap with the output value $y$ of the perceptron to different input values $(x_0, x_1)$. As indicated on the color bar next to the graph, values for $(x_0, x_1)$ that fall in the purple region map to an output value of $y=0$. Those falling in the yellow region map to an output $y=1$.\n",
    "\n",
    "We add some annotated samples to the plot to see in which region they fall. Their fill color shows their true class and their edge color indicates whether or not the perceptron classifies the sample correctly. **Can you change the perceptron weights and bias such that all training samples are classified correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18995ac4-e0e4-41e8-b9ca-642da7b1883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A batch of (x0, x1) inputs\n",
    "x = tensor([[2., 2.],\n",
    "            [1., -2.],\n",
    "            [-2., 2.],\n",
    "            [-1., 1.]])\n",
    "\n",
    "# A batch of desired outputs y\n",
    "y = tensor([[0.],\n",
    "            [1.],\n",
    "            [0.],\n",
    "            [1.]])\n",
    "\n",
    "# Create a perceptron\n",
    "ptron = create_perceptron()\n",
    "\n",
    "# Modify the weights and bias values\n",
    "ptron.weights = [torch.tensor([[1., 1.]])]\n",
    "ptron.biases = [torch.tensor([0.])]\n",
    "\n",
    "# Visualize the response and the architecture\n",
    "resp_view = MLPResponseView(ptron, data_x=x, data_y=y)\n",
    "arch_view = MLPArchView(ptron)\n",
    "display(arch_view, resp_view.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb50493-ca9c-458e-8c11-258f706d107d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Implement your own perceptron\n",
    "\n",
    "We will now implement our own perceptron in PyTorch. To do this, we need to implement a class that **inherits from `torch.nn.Module`**. The class must implement a method called `forward()` that takes in the network input and returns the network output. A perceptron linearly combines its input, adds a bias and passes this through a hard-limit activation.\n",
    "\n",
    "- For the linear combination (with bias), you can create an instance of [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). With the constructor arguments `in_features` (int), `out_features` (int) and `bias` (bool), you can control the number of input neurons, output neurons and whether or not a bias should be added, respectively.\n",
    "- We have already implemented the hard-limit \"layer\" for you.\n",
    "\n",
    "**Copy the code below and paste it into a code cell and complete it to create your first perceptron**!\n",
    "\n",
    "```python\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, num_input_neurons):\n",
    "        super().__init__()\n",
    "        self.layer0 = # An nn.Linear layer that transforms num_input_neurons to 1 output neuron and adds a bias\n",
    "        self.hlim = HardLimit()\n",
    "\n",
    "        # Store this for easy access\n",
    "        self.num_input_neurons = num_input_neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        y = self.hlim(x)\n",
    "        return y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2814e1-ba95-4d00-b985-8d0c53540c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paste the code of your perceptron in this cell ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db159b31-fe2d-4425-a7c4-9ebc723466ec",
   "metadata": {},
   "source": [
    "Now, you can create a perceptron with 2 input neurons by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f0b2b-df07-4d51-8138-ea46eee6d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron = Perceptron(\n",
    "    num_input_neurons=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125f814-633a-4b0e-8de9-66f0cdff1307",
   "metadata": {},
   "source": [
    "The parameters of your perceptron are inside `ptron.layer0`. As such, you can print the weights and bias with `ptron.layer0.weight` and `ptron.layer0.bias`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ece83-67ad-4977-a8d2-5ce5ec400262",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron.layer0.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da469d-983a-452c-9207-24417355150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron.layer0.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c42531-bc4b-4cfd-8225-b24225cfc115",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Training the perceptron\n",
    "\n",
    "Instead of searching the perceptron parameters manually, it is much easier to let the computer find the optimal weights and bias automatically. This can be done with the **perceptron learning rule** or the **normalized perceptron learning rule**. We have already implemented both of these below. As you can see, in the vanilla perceptron learning rule, we compute a `dw` and `db` and add them to the perceptron weights and bias, respectively. The normalized perceptron learning rule only differs from the other rule in that it normalizes the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a234f6-531f-498c-861b-5dbc674196b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def perceptron_learning_rule(ptron, x_sample, y_sample, normalize=False, lr=0.5):\n",
    "    \"\"\"\n",
    "    The perceptron training rule (vanilla and normalized).\n",
    "\n",
    "    Args:\n",
    "        ptron (Perceptron): The perceptron to train.\n",
    "        x_sample (torch.Tensor): A sample of the input training data.\n",
    "        y_sample (torch.Tensor): The desired output.\n",
    "        normalize (bool): (optional) If True, use the normalized learning rule.\n",
    "        lr (float): (optional) The learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the output of the input training sample\n",
    "    y_pred = ptron(x_sample)\n",
    "\n",
    "    # x_sample with 1.0 appended for bias\n",
    "    x_sample_b = torch.cat([x_sample, torch.tensor([1.0])])\n",
    "\n",
    "    # If the prediction is false negative, add x to w\n",
    "    # If the prediction is false positive, subtract x from w\n",
    "    if normalize:\n",
    "        x_sample_b = x_sample_b/x_sample_b.norm()\n",
    "\n",
    "    dw = lr*(y_sample - y_pred)*x_sample_b[:-1]\n",
    "    ptron.layer0.weight += dw\n",
    "\n",
    "    # Do the same for the bias.\n",
    "    db = lr*(y_sample - y_pred)*x_sample_b[-1]\n",
    "    ptron.layer0.bias += db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7cf07-becb-4b87-8738-da4d3e8a451e",
   "metadata": {},
   "source": [
    "We will train the perceptron by iteratively applying the learning rule implemented above. During each epoch, we iterate over all samples in the training dataset and apply the perceptron learning rule during each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9afb57-6448-44c6-89df-7ce7c0df0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000  # Choose the maximum number of training epochs\n",
    "converged = False  # This will be set to True when the perceptron has converged\n",
    "\n",
    "\n",
    "for i in range(max_epochs):\n",
    "\n",
    "    for x_sample, y_sample in zip(x, y): # Iterate over all training samples\n",
    "\n",
    "        # Update the perceptron\n",
    "        perceptron_learning_rule(ptron, x_sample, y_sample)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        acc = get_accuracy(ptron, x, y)\n",
    "\n",
    "        if acc == 1:  # Are all trainig samples classified correctly?\n",
    "            # If so, our perceptron has converged!\n",
    "            print(f\"Perceptron converged! 🙌 (took {i + 1} epochs)\")\n",
    "            converged = True\n",
    "\n",
    "            # Break out of the loop\n",
    "            break\n",
    "\n",
    "    if converged:\n",
    "        break\n",
    "\n",
    "if not converged:\n",
    "    # If the training did not converge, print a \n",
    "    print(f\"Training finished without converging... 🙄 (after {i + 1} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb73a77-46ea-45f1-8b72-4f4c8e70d1c6",
   "metadata": {},
   "source": [
    "If you have correctly implemented everything, your perceptron should have converged without any problems. You can see your perceptron in action with the `MLPResponseView`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea363e-e225-4582-998b-0671b228ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron_view = MLPResponseView(\n",
    "    ptron,\n",
    "    data_x=x,\n",
    "    data_y=y\n",
    ")\n",
    "ptron_view.fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10042589-2c22-4795-8d3f-5d112c67bb77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Logging a training metric with TensorBoard\n",
    "\n",
    "We executed our training loop above in a rather blindfolded way. We have no idea how the accuracy of our model evolved as the training proceeded. Instead, it is customary to regularly log one or more metrics while training. We can use **TensorBoard** for this.\n",
    "\n",
    "To log the accuracy to TensorBoard, we need a [`SummaryWriter`](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter) instance. You can import the `SummaryWriter` class like so:\n",
    "\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "```\n",
    "\n",
    "After which you can create a `SummaryWriter` instance. Although optional, it might come in handy to pass in a meaningful value for `log_dir` with some short info on your run (like which learning rule you used).\n",
    "\n",
    "```python\n",
    "writer = SummaryWriter(log_dir=\"my_run_dir/<put some useful info here>\")\n",
    "```\n",
    "\n",
    "To log the accuracy value (stored in `acc`) that our perceptron had at epoch `epoch`, you do\n",
    "\n",
    "```python\n",
    "writer.add_scalar(\"Accuracy/Train\", acc, epoch)\n",
    "```\n",
    "\n",
    "The `\"Accuracy/train\"` is the name of the graph to which you will log the scalar. You can use a forward slash (`/`) to put the graph inside a named *section* in TensorBoard. In this case, our accuracy data will be added to the graph *Train* inside a section called *Accuracy*.\n",
    "\n",
    "We have created a new function for you called `perceptron_training_loop()` that implements the same training logic as before, but now also logs the training accuracy to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a5dff-47d9-4a73-be60-e655bafadf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_training_loop(ptron, x, y, normalize=False, max_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train a perceptron until convergence.\n",
    "\n",
    "    Args:\n",
    "        ptron (Perceptron): The perceptron model.\n",
    "        x (torch.Tensor): All input training samples.\n",
    "        y (torch.Tensor): All corresponding ground truth output samples.\n",
    "        max_epochs (int): The maximum number of epochs.\n",
    "        normalize (bool): (optional) If True, use the normalized learning rule.\n",
    "    \"\"\"\n",
    "    # Create a SummaryWriter instance with a meaningful run name\n",
    "    timestamp = datetime.now().isoformat()  # We will append a timestamp to make the run name unique\n",
    "    run_name = f'{\"normalize\" if normalize else \"regular\"}_{timestamp}'\n",
    "    writer = SummaryWriter(log_dir=f'runs_perceptron/{run_name}')\n",
    "\n",
    "    converged = False\n",
    "\n",
    "    for i in range(max_epochs):\n",
    "\n",
    "        for x_sample, y_sample in zip(x, y):  # Iterate over all training samples\n",
    "\n",
    "            # Update the perceptron\n",
    "            perceptron_learning_rule(ptron, x_sample, y_sample,\n",
    "                                     normalize=normalize)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = get_accuracy(ptron, x, y)\n",
    "\n",
    "            if acc == 1:  # Are all trainig samples classified correctly?\n",
    "                # If so, our perceptron has converged!\n",
    "                print(f\"Perceptron converged! 🙌 (took {i + 1} epochs)\")\n",
    "                converged = True\n",
    "\n",
    "                # Break out of the loop\n",
    "                break\n",
    "\n",
    "        # Log the training accuracy\n",
    "        writer.add_scalar(\"Accuracy/Train\", acc, i)\n",
    "\n",
    "        if converged:\n",
    "            return\n",
    "\n",
    "    if not converged:\n",
    "        # If the training did not converge, print a \n",
    "        print(f\"Training finished without converging... 🙄 (after {i + 1} iters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065c5f1-abc3-4f9c-8f1a-bc45b4baec98",
   "metadata": {},
   "source": [
    "To start TensorBoard on VSC OnDemand, go to [the dashboard](https://ondemand.hpc.kuleuven.be/pun/sys/dashboard/) and click on \"TensorBoard\". Use the following settings:\n",
    "\n",
    "- Number of cores: 1\n",
    "- Account: lp_wice_pilot\n",
    "- Partition: batch\n",
    "- Project/Log folder: anndl_es1_files/es1/runs_perceptron\n",
    "- Number of hours: 16\n",
    "- Number of gpu's: 0\n",
    "\n",
    "Leave the other settings at their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295179aa-73a3-434b-8f44-4faa6635809f",
   "metadata": {},
   "source": [
    "Before we continue, let's reinitialize the weights and bias of the perceptron to some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee15634-9d30-417d-a4fb-1a69867a5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron.layer0.weight.data = torch.rand([1, 2])     # Tensor of shape [1, 2] with random values between [0, 1]\n",
    "ptron.layer0.bias.data   = torch.rand([1])*2 - 1  # Tensor of shape [1] with random value between [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02dd6c-bb2f-4f52-822e-4c939362346e",
   "metadata": {},
   "source": [
    "Now call our training loop function and push the refresh button in TensorBoard to see the progress of the accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d967-55b0-4ea8-971a-3c91af724995",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_training_loop(ptron, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffe5c2-f4e1-4d36-b53a-1aed46bdafda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4 Exercise\n",
    "\n",
    "Train a perceptron with each of the following datasets. Visualize your results. Can you always train it perfectly? If not, why?\n",
    "\n",
    "Don't forget to reinitialize the perceptron weights for each new training!\n",
    "\n",
    "```python\n",
    "x1 = tensor([[-0.5, -0.5],\n",
    "             [-0.5,  0.5],\n",
    "             [ 0.3,  -0.5],\n",
    "             [-0.1,  0.1]])\n",
    "y1 = tensor([[1.],\n",
    "             [1.],\n",
    "             [0.],\n",
    "             [0.]])\n",
    "\n",
    "x2 = tensor([[-0.5, -0.5],\n",
    "             [-0.5,  0.5],\n",
    "             [ 0.3,  -0.5],\n",
    "             [-0.1,  0.1],\n",
    "             [-40.,   50.]])\n",
    "y2 = tensor([[1.],\n",
    "             [1.],\n",
    "             [0.],\n",
    "             [0.],\n",
    "             [1.]])\n",
    "\n",
    "x3 = tensor([[-0.5, -0.5],\n",
    "             [-0.5,  0.5],\n",
    "             [ 0.3,  -0.5],\n",
    "             [-0.1,  0.1],\n",
    "             [-0.8,  0.0]])\n",
    "y3 = tensor([[1.],\n",
    "             [1.],\n",
    "             [0.],\n",
    "             [0.],\n",
    "             [0.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11e5b0-6c11-4062-a804-ca4b32b61429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
