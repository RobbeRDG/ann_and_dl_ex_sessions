{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ffb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from lib.data.pseudo_corr import get_corr_samples, get_uncorr_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945add6",
   "metadata": {},
   "source": [
    "# 1. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique for **dimensionality reduction** of a dataset. The idea is to map a vector $x = (x_1, x_1,\\ldots,x_m)$ to a lower-dimensional vector $z = (z_1,z_2,\\ldots,z_n)$ with $n\\lt m$. We are going to consider linear mappings here, such that\n",
    "\n",
    "$$\n",
    "z = Px\n",
    "$$\n",
    "\n",
    "with $P$ a matrix of size $m\\times m$. In PCA, this matrix $P$ is composed by choosing the $n$ eigenvectors that correspond to the $n$ largest eigenvalues of the covariance matrix of the dataset. These eigenvectors indicate the directions in which the data varies the most. When we project the data onto these eigenvectors (which is exactly what we do when multiplying with matrix $P$) we will thus maximally preserve the data variability in the lower-dimensional space.\n",
    "\n",
    "More specifically, one can use the following algorithm to perform PCA reduction:\n",
    "\n",
    "- Calculate the $m\\times m$ dimensional covariance matrix of the dataset.\n",
    "- Calculate the eigenvectors and eigenvalues of this covariance matrix.\n",
    "- Determine the dimension $n$ of the reduced dataset by looking at the largest eigenvalues. The quality of the reduction depends on how close the sum of the largest $n$ eigenvalues is to the sum of all $m$ eigenvalues.\n",
    "- Create the $n \\times m$ projection matrix $P$ from the eigenvectors corresponding to the $n$ largest eigenvalues\n",
    "- Before transforming the data, we should shift its center to the origin. To do this, compute the mean of each of the $m$ dimensions and subtract these from all datapoints.\n",
    "- Reduce the centered dataset by multiplying it with $P$.\n",
    "\n",
    "Additionally, we can try to *reconstruct* our original dataset from the compressed data. We multiply the compressed data with an $m\\times n$-sized matrix $Q$:\n",
    "\n",
    "$$\n",
    "\\hat{x} = Qz = QPx.\n",
    "$$\n",
    "\n",
    "We choose $Q$ to be equal to the transpose of $P$, i.e., $Q = P^T$. If $n$ is chosen well, these regenerated datapoints should be fairly similar to the original datapoints, thus capturing most of the information in the dataset. Remember to shift the reconstructed data back to the original center of the dataset by adding the mean again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f97c98",
   "metadata": {},
   "source": [
    "## 1.1 Exercise: Redundancy and Random Data\n",
    "\n",
    "### 1.1.1 Implement Your Own PCA Algorithm\n",
    "\n",
    "The idea of this exercise is to write your own function that implements the PCA algorithm in PyTorch. The above algorithm can be programmed in PyTorch with the following functions:\n",
    "\n",
    "- [`torch.cov()`](https://pytorch.org/docs/stable/generated/torch.cov.html): Calculates the sample covariance matrix of the given data. The data is expected to have shape $m\\times B$, where the $m$ **rows are the features** and the $B$ **columns are the samples**.\n",
    "- [`torch.linalg.eigh()`](https://pytorch.org/docs/stable/generated/torch.linalg.eigh.html): Computes the eigenvalues and eigenvectors of a symmetric matrix. One could also use [`torch.linalg.eig()`](https://pytorch.org/docs/stable/generated/torch.linalg.eig.html), but as a covariance matrix is symmetric by definition, it is better to use `torch.linalg.eigh()`, as this is optimized for symmetric matrices.\n",
    "- [`torch.argsort()`](https://pytorch.org/docs/stable/generated/torch.argsort.html): Returns the indices that sort a tensor. Use the argument `descending=True` to get indices that sort in descending order.\n",
    "- [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html): Computes the product of two matrices.\n",
    "\n",
    "Some interesting tensor operations:\n",
    "\n",
    "- Transpose a matrix `X`: [`X.T`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T)\n",
    "- Compute the mean of the items in a matrix `X` along a given dimension: [`X.mean()`](https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean)\n",
    "- Compute the sum of the items in a matrix `X` along a given dimension: [`X.sum()`](https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum)\n",
    "\n",
    "Make sure to keep track of the shapes of the matrices you are working with. For the functions you use, make sure you check the meaning of each dimension of the function's input. Transpose your matrices when necessary.\n",
    "\n",
    "We'll start by **implementing a function that computes the eigenvalues and the eigenvectors of the covariance matrix of a dataset**. Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1840f-c804-4227-99ac-4d310dbf7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eig_cov(X):\n",
    "    \"\"\"\n",
    "    Compute the eigenvalues and -vectors of the covariance matrix of X.\n",
    "    \n",
    "    Args:\n",
    "        X (torch.Tensor): Tensor of shape B x m where the rows (dim 0) are observations\n",
    "            and the columns (dim 1) are variables.\n",
    "    Returns:\n",
    "        Tuple containing eigenvalues and eigenvectors with the eigenvectors in the\n",
    "            columns.\n",
    "    \"\"\"\n",
    "    # Estimate the m x m covariance matrix\n",
    "    cov_mtrx = ???\n",
    "\n",
    "    # Compute the eigenvectors and -values of the cov matrix\n",
    "    eig_vals, eig_vecs = ???\n",
    "\n",
    "    return eig_vals, eig_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ffd54-cd37-4c79-8315-2ed7ccbd4ea9",
   "metadata": {},
   "source": [
    "To compose the transformation matrix $P$, we need the $n$ eigenvectors that correspond to the $n$ largest eigenvalues. As such, let's write a function that **takes in a set of eigenvalues and corresponding eigenvectors and sorts them in decreasing order of eigenvalues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8267ae-d447-4776-8f5a-72da531dddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_eig(eig_vals, eig_vecs):\n",
    "    \"\"\"\n",
    "    Sort the eigenvalues and -vectors in decreasing eigenvalue order.\n",
    "    \n",
    "    Args:\n",
    "        eig_vals (torch.Tensor): Tensor of shape \n",
    "    \"\"\"\n",
    "    # Get the inidices that sort the eigenvalues from high to low\n",
    "    idxs = ???\n",
    "\n",
    "    # Sort the eigenvalues and eigenvectors using these indices\n",
    "    sorted_eig_vals = eig_vals[idxs]\n",
    "    sorted_eig_vecs = eig_vecs[:, idxs]\n",
    "\n",
    "    return sorted_eig_vals, sorted_eig_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16927c3",
   "metadata": {},
   "source": [
    "For the PCA implementation itself, we'll write a class that computes the eigenvalues and eigenvectors of the dataset in its constructor (the `__init__()` function) and performs a PCA of this dataset for any dimension `n` in the method `run()`. **Complete the implementation below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad962136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.Tensor): Tensor containing the B samples that constitute\n",
    "                the dataset.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "\n",
    "        # Flatten all dataset samples (e.g. images) into B vectors of length m\n",
    "        self.orig_shape = X.shape[1:]\n",
    "        self.X_flat = X.flatten(start_dim=1).float()\n",
    "\n",
    "        # Compute the eigenvalues and eigenvectors of X_flat\n",
    "        eig_vals, eig_vecs = ???\n",
    "\n",
    "        # Sort the eigenvalues and eigenvectors according to decreasing eigenvalues\n",
    "        self.eig_vals, self.eig_vecs = ???\n",
    "\n",
    "        # Compute the mean of all data samples\n",
    "        self.X_mean = self.X_flat.mean(dim=0)[None, :]\n",
    "\n",
    "    def run(self, n):\n",
    "        # Get the n largest eigenvectors as an n x m matrix\n",
    "        self.P = ???\n",
    "\n",
    "        # Shift center of X to origin before transforming\n",
    "        X_center = ???\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        # Think about the shapes of the matrices!\n",
    "        self.Z = ???\n",
    "\n",
    "        # Reconstruct X from encodings\n",
    "        X_rec = ???\n",
    "\n",
    "        # Shift center back to original\n",
    "        X_rec += self.X_mean\n",
    "\n",
    "        # Reshape to original shape\n",
    "        self.X_rec_flat = X_rec\n",
    "        self.X_rec = X_rec.unflatten(1, self.orig_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00b404",
   "metadata": {},
   "source": [
    "Given a dataset `X` with `B` samples, we can create a PCA instance as follows:\n",
    "\n",
    "```python\n",
    "pca = PCA(X)\n",
    "```\n",
    "\n",
    "To apply PCA to the dataset with a dimensionality reduction to a dimension `n`, we call `run()` on the `pca` object:\n",
    "\n",
    "```python\n",
    "pca.run(n)\n",
    "```\n",
    "\n",
    "After that, `pca.Z` contains `B` vectors of (lower) dimension `n` and `pca.X_rec` contains `B` reconstructions of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4249",
   "metadata": {},
   "source": [
    "### 1.1.2 Reduce Dimensionality of Uncorrelated Data\n",
    "\n",
    "Use the function `get_uncorr_samples(n_samples)` to generate 500 samples of dimension 2 and reduce the samples to dimension 1 with your PCA implementation. Try to reconstruct the original data and estimate the error, e.g., by computing the root mean square of the difference between the reconstructed and the original data:\n",
    "\n",
    "```python\n",
    "((X - X_rec)**2).mean().sqrt()\n",
    "```\n",
    "\n",
    "Visualize the original data and the reconstructed data in a plot:\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(*X.T)\n",
    "ax.scatter(*X_rec.T)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7c6eb",
   "metadata": {},
   "source": [
    "### 1.1.3 Reduce Dimensionality of Correlated Data\n",
    "\n",
    "Do the same for 500 samples returned by `get_corr_samples(n_samples)`. How does the reduction of uncorrelated data compare to the reduction of correlated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85812c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0ba80",
   "metadata": {},
   "source": [
    "## 1.2 Principal Component Analysis on Handwritten Digits\n",
    "\n",
    "### 1.2.1 The MNIST Dataset\n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset containing images of handwritten digits. We can easily get the dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(root='data', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2c1e9",
   "metadata": {},
   "source": [
    "The dataset will automatically be downloaded in a folder called `data`. The `mnist` object has an attribute `data` that contains the raw images as PyTorch tensors. For example, to show the image at index `0`, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist.data[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b628f4e",
   "metadata": {},
   "source": [
    "The `cmap='gray'` argument ensures that the image is visualized in grayscale.\n",
    "\n",
    "The `mnist` object also has an attribute `targets` that contains the label of each image in `mnist.data`. For example, with the follwing code, we can select all images that have the label `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.nonzero(mnist.targets == 3).flatten()\n",
    "threes = mnist.data[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd4088",
   "metadata": {},
   "source": [
    "Compute the mean 3 and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce79335",
   "metadata": {},
   "source": [
    "Before applying PCA, you should flatten the images such that they are all vectors and cast the values to floats. You can do this with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c92b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = threes.flatten(start_dim=1).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5f8e5",
   "metadata": {},
   "source": [
    "To visualize a batch of reconstructed images `X_rec`, you'll need to reshape the images back into their original shape with `X_rec.unflatten(1, (28, 28))`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e061c63",
   "metadata": {},
   "source": [
    "### 1.2.2 Exercise: Apply PCA to MNIST\n",
    "\n",
    "- Apply your PCA implementation to compress the dataset of threes by projecting it onto one, two, three and four principal components. Visualize some reconstructions of each compression.\n",
    "- Compress the data now for $n \\in \\{1,2,\\ldots, 50 \\}$ and compute the reconstruction error for each value of $n$. You'll want to use a `for` loop here. Also see the built-in Python [`range()`](https://www.w3schools.com/python/ref_func_range.asp) function. Plot the reconstruction error as a function of $n$.\n",
    "- What should the reconstruction error be if $n = 784$? What is it if you actually try it? Why?\n",
    "- Use [`torch.cumsum()`](https://pytorch.org/docs/stable/generated/torch.cumsum.html) to create a vector whose $i$-th element is the sum of all but the $i$ largest eigenvalues with $i \\in \\{1,2\\ldots, 784\\}$. Visualize this with Matplotlib using `plt.plot()`. Compare the first 50 elements of this vector to the vector of reconstruction errors calculated previously. What do you notice?\n",
    "\n",
    "The last question should show you a very interesting and important fact: the squared reconstruction error induced by not using a certain principal component is proportional to its eigenvalue. That is why, if the eigenvalues fall off quickly, projecting onto the first few components gives small errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80377cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f299b0a30a695930222e2b85814db2feaa48d467a39d54db5a092acf936d7d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
