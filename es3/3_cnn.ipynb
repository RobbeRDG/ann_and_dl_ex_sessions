{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855255f-41de-43de-9698-5f5b8580e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms as tfm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.data.baltech_101 import Baltech101\n",
    "from lib.utils.dataset_features import get_dataset_features\n",
    "from lib.utils.feature_hooks import add_feature_hooks\n",
    "from lib.utils.image_tools import show_ims\n",
    "from lib.utils.model_device import get_model_device\n",
    "from lib.utils.run_name import get_clf_run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdcdf5-109f-41a1-ba6e-40053e2b653c",
   "metadata": {},
   "source": [
    "# 3. Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a deep learning technique that uses the concept of *local connectivity*. The idea is that in a lot of datasets, tensor elements that are close to each other, are likely to be a lot more connected than tensor elements that are further away. For example, the elements in an image tensor represent pixels. Pixels that are spatially close to each other are likely to represent the same part of the image, while pixels that are further away can represent different parts.\n",
    "\n",
    "CNNs are explained in [this Stanford tutorial](http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/).\n",
    "\n",
    "## 3.1 Classifying MNIST with a CNN\n",
    "\n",
    "In this section, we are going to train a basic CNN on the MNIST dataset. Let's first take a look at how we can implement a CNN in PyTorch.\n",
    "\n",
    "### 3.1.1 Implementing a Simple CNN\n",
    "\n",
    "You can use [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) to create a single convolutional layer in PyTorch. Some of its interesting arguments are:\n",
    "\n",
    "- `in_channels`: the number of channels in the input tensor. When the input is an RGB image, this is `3`. For a single-channel gray scale image, this is `1`. Note that each convolution filter in the layer will have this amount of channels as the filter needs to cover the *entire depth* of the input.\n",
    "- `out_channels`: the number of convolution filters that this layer has. Each filter will produce a new channel in the output. As such, if another `nn.Conv2d()` takes the output of this layer as its input, that layer's `in_channels` should be set to this layer's `out_channels`.\n",
    "- `kernel_size`: the height and width of the convolution filters. If this is a single integer, it will be used for both the height and the width (i.e., square filters).\n",
    "- `stride`: the stride (step size) of the convolution operation (default: `1`).\n",
    "- `padding`: the amount of padding to add to the input (default: `0`). *Padding* is a border of black pixels that is added around the input image. This allows more of the convolution operation to be applied to the pixels at the edge of the image and can avoid the output resolution to shrink w.r.t. the input resolution. When `padding` is set to `1`, a black border of a single pixel wide will be added at each image edge.\n",
    "\n",
    "Apart from convolutional layers, a CNN also typically contains *pooling layers*. Similar to conv layers, a pooling layer uses a sliding window to operate on its input. Instead of computing an inner product, however, the pooling window **aggregates** the underlying values, e.g., by computing the *maximum* or *average* value. For example:\n",
    "\n",
    "- [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) applies max pooling.\n",
    "- [`nn.AvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) applies average pooling.\n",
    "\n",
    "Just like `nn.Conv2d()`, these pooling layers take `kernel_size`, `stride` (default: `1`) and `padding` (default: `0`) as an argument.\n",
    "\n",
    "The pooling layers also have *adaptive* equivalents: [`nn.AdaptiveMaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html) and [`nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html). These adaptive layers produce an **output of a fixed, predefined width and height no matter the input size** (the number of channels stays the same). This is in contrast with regular pooling layers, where the output size depends on the input size.\n",
    "\n",
    "**Copy-paste the following CNN implementation in the cell below and complete it.** Note that we did not always specify the number of input channels and input features, as you should be able to infer these from the previous layers.\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = # conv with 12 5x5 conv filters with 1 input channel\n",
    "        self.relu1 = # ReLU activation\n",
    "        self.pool1 = # max pooling with kernel size 2 and stride 2\n",
    "\n",
    "        self.conv2 = # conv with 24 5x5 conv filters\n",
    "        self.relu2 = # ReLU activation\n",
    "        self.pool2 = # adaptive avg pool with output width and height 1\n",
    "\n",
    "        # This flattens the output of pool2 to N x 24 (N = batch size)\n",
    "        self.flatten = nn.Flatten(\n",
    "            start_dim=1\n",
    "        )\n",
    "        self.fc = # fully-connected layer that outputs 10 values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841b4ed-3159-4cb5-8ea7-840fd564445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your CNN implementation in this cell ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23148d8-6c4d-49a5-a4a2-3c0cd32d771b",
   "metadata": {},
   "source": [
    "For each input, your CNN returns 10 values. These values will correspond to the 10 classes in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5500c18-cc57-4b8e-917e-78681fa3fda4",
   "metadata": {},
   "source": [
    "### 3.1.2 Preparing the Data for Mini-Batch Training\n",
    "\n",
    "In the previous notebook, we passed *the entire* MNIST dataset in batch through the model during training. This is possible because each MNIST image consists of only a single $28\\times 28$ channel and there are *only* 60 000 training images in the dataset. In real-world computer vision applications, the images are typically much larger in size and number, making it infeasible or impossible to train with batches containing the entire training dataset.\n",
    "\n",
    "Instead, it is customary to train CNNs on **mini-batches**. Each such mini-batch contains only a fraction of all samples available in the training dataset. After each forward pass with a mini-batch, the parameters of the model are updated. When the model has seen all training samples, a single epoch is done. Before each of the next epoch(s), the data can be shuffled such that mini-batches always contain other combinations of samples.\n",
    "\n",
    "Before diving into mini-batch training with PyTorch, let's first learn about PyTorch **[`Dataset`s](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)**. A PyTorch `Dataset` contains all the information that you need to get a sample from a certain dataset. The `MNIST` class, for example, is a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4c147-d095-4c06-80dc-813b69352a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\n",
    "    root='data',\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "isinstance(mnist_train, Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c82e9-b3bc-4a03-a12f-2568e348719a",
   "metadata": {},
   "source": [
    "Since `mnist_train` is an instance of a PyTorch `Dataset`, it is **guaranteed** that `mnist_train` supports the following two operations:\n",
    "\n",
    "1. `len(mnist_train)` returns the total number of samples in the dataset;\n",
    "2. `mnist_train[i]` returns the data sample at index `i`, with `i >= 0` and `i < len(mnist_train)`.\n",
    "\n",
    "Indeed, `mnist_train` supports these operations in an exemplary way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389aeeb-3228-441e-8a2f-70b35c5b6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c40cbd-e4b3-47b6-8f76-ec5b350ea702",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "mnist_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdea574-1ea8-4b44-82df-4ffb0f4ea1be",
   "metadata": {},
   "source": [
    "As you can see, choosing a random sample from `mnist_train` returns a tuple containining a `PIL.Image` and the label of the corresponding image. Unfortunately, we cannot use a `PIL.Image` as input for our model. The `PIL.Image` must be transformed into a PyTorch `Tensor`. We can **pass in an argument `transform` to the `MNIST` constructor** to take care of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebaa75b-aabf-4aee-b160-62817956a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\n",
    "    root='data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=tfm.ToTensor()  # This transforms the PIL.Image into a Tensor when sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4d210-530f-4a04-80c8-564f98d4e6c3",
   "metadata": {},
   "source": [
    "Indeed, now `mnist_train[i]` returns a PyTorch `Tensor`! ðŸ™Œ (along with the corresponding label)\n",
    "\n",
    "> Note that you can pass in much more advanced transforms as well. More specifically, if you want to perform *data augmentation*, you typically pass in a chain of transforms with [`Compose()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). For more information, see [here](https://pytorch.org/vision/stable/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb7c55-0cca-4552-8567-49038b87a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = mnist_train[i]\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1172e3e-2e6b-48dd-b167-75e77179d644",
   "metadata": {},
   "source": [
    "Now, how can we use a `Dataset` to create mini-batches for training? For that, we need a PyTorch [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). A `DataLoader` samples from the given dataset and combines the samples into batches of a predefined batch size. Let's create a `DataLoader` for `mnist_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac30f8-bb5f-41e1-9ea7-d95da2b3af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    mnist_train,      # The dataset\n",
    "    batch_size=4096,  # How many samples should be inside a single batch?\n",
    "    shuffle=True,     # If True, compose the batches randomly\n",
    "    num_workers=4,    # The number of parallel jobs to use during data loading\n",
    "    drop_last=False,  # If True, drop the last incomplete batch (if the dataset size is not divisible by the batch size).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb54903-73f5-40e1-adf9-ca10f159d0ad",
   "metadata": {},
   "source": [
    "We can **put `dl_train` in a `for` loop** to iterate over all batches of a single epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316947c-56c5-4a72-81f9-d532957830aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_train, y_train in dl_train:\n",
    "    print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b1edf-c1b1-4614-b81e-cc3bca785348",
   "metadata": {},
   "source": [
    "As you can see, we get 15 batches: 14 batches with 4096 samples and 1 batch with 2656 samples, totaling to 60 000 samples.\n",
    "\n",
    "Let's create a data loader for the validation data as well. It is best to set `shuffle=False` so that each validation loop uses the exact same batches. For simplicity, we use the entire dataset size as the batch size for validation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14dff2-3180-472c-84ee-91ab55a1956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_val = MNIST(\n",
    "    root='data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=tfm.ToTensor()\n",
    ")\n",
    "\n",
    "dl_val = DataLoader(\n",
    "    mnist_val,\n",
    "    batch_size=len(mnist_val),\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0680d-6f9c-44aa-8b9c-970b2b3ef273",
   "metadata": {},
   "source": [
    "### 3.1.3 Defining the Training Loop\n",
    "\n",
    "Now that we have our model and data ready, we can define our training loop! We have already implemented it below. The implementation is very similar to the previous training loops we have seen. There are a couple of notable differences, however:\n",
    "\n",
    "- There are *two nested `for` loops* that iterate over the dataloaders;\n",
    "- We keep track of a *global training and validation iteration step* to have unique values for the horizontal axes in TensorBoard;\n",
    "- We move the batches to the compute device (GPU) in the training script itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83818b2e-295a-4e4c-8cb7-46eb6715993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, dl_train, dl_val, optimizer, loss_fn, num_epochs, log_root):\n",
    "    \"\"\"\n",
    "    Train a classifier.\n",
    "\n",
    "    Args:\n",
    "        model: The classification model.\n",
    "        dl_train: The training data loader.\n",
    "        dl_val: The validation data loader.\n",
    "        optimizer: The optimizer.\n",
    "        loss_fn: The loss function.\n",
    "        num_epochs: The number of epochs.\n",
    "        log_root: The root directory of the TensorBoard logs.\n",
    "    \"\"\"\n",
    "    # Get compute device of model\n",
    "    device = get_model_device(model)\n",
    "\n",
    "    # Create SummaryWriter with a meaningful name\n",
    "    run_name = get_clf_run_name(model, optimizer, dl_train)\n",
    "    writer = SummaryWriter(log_dir=f'{log_root}/{run_name}')\n",
    "\n",
    "    # Global training and validation iteration step\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    # Iterate over the epochs\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Put model in training mode (see Sec. 3.2.1)\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over the training mini-batches\n",
    "        for x_train, y_train in dl_train:\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "\n",
    "            y_pred = model(x_train)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "            # Backpropagate + optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss\n",
    "            writer.add_scalar(f\"{loss_fn.__class__.__name__}/Train\", loss, train_step)\n",
    "            writer.add_scalar(\"Epoch/Train\", epoch, train_step)\n",
    "\n",
    "            train_step += 1\n",
    "\n",
    "        # Put model in evaluation mode (see Sec. 3.2.1)\n",
    "        model.eval()\n",
    "\n",
    "        # Iterate over the validation mini-batches\n",
    "        for x_val, y_val in dl_val:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_val)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(y_pred, y_val)\n",
    "\n",
    "                # Compute accuracy\n",
    "                acc = (y_val == y_pred.argmax(dim=1)).float().mean()\n",
    "\n",
    "            # Log validation loss and accuracy\n",
    "            writer.add_scalar(f\"{loss_fn.__class__.__name__}/Val\", loss, val_step)\n",
    "            writer.add_scalar(\"Accuracy/Val\", acc, val_step)\n",
    "            writer.add_scalar(\"Epoch/Val\", epoch, val_step)\n",
    "\n",
    "            val_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486c3b3-3165-4c25-add0-3e57f4bd788a",
   "metadata": {},
   "source": [
    "### 3.1.4 Running the Training\n",
    "\n",
    "Time to train our home-grown CNN! ðŸª´\n",
    "\n",
    "- Create a CNN called `simple_cnn` by instantiating your `SimpleCNN` class;\n",
    "- Create a suitable optimizer;\n",
    "- Define a suitable loss function for training classification;\n",
    "- Also redefine the training data loader in the cell below, so that you can easily play with the batch size. Keep the validation batch size fixed, though, no matter what training batch size you choose;\n",
    "- Don't forget to move your model to the correct compute device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code in this cell ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1891fd-d710-4a6a-9c42-31aef3115e75",
   "metadata": {},
   "source": [
    "You can use the following cell to visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Compute class logits of each sample\n",
    "    y_pred = torch.cat([\n",
    "        simple_cnn(x_val.to(device))\n",
    "        for x_val, _ in dl_val\n",
    "    ])\n",
    "\n",
    "# For each sample, get the class with the highest logit\n",
    "y_pred = y_pred.argmax(dim=1).cpu()\n",
    "\n",
    "# Get the ground-truth classes\n",
    "y_true = mnist_val.targets\n",
    "\n",
    "# Compute and show the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b577e-9cf3-4751-ba7d-c2dfaf58dc77",
   "metadata": {},
   "source": [
    "### 3.1.5 Exercise\n",
    "\n",
    "- Try out different values for the training batch size. What do you notice? Briefly discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ba4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e4d0470-59e4-4af9-901d-ee1d0ff68048",
   "metadata": {},
   "source": [
    "## 3.2 Using a Pre-Trained CNN as Deep Feature Extractor\n",
    "\n",
    "In the \"old\" days of computer vision, researchers put a lot of effort in developing hand-crafted algorithms that could extract descriptive *features* from an image. Typical examples of such algorithms are SIFT, SURF and HOG. These general features can be used for a down-stream task like classification, which would be trained with a machine learning algorithm like SVM, taking the features as an input.\n",
    "\n",
    "Since the advent of deep learning, CNNs have shown impressive capabilities as general feature extractors, often outperforming hand-crafted features by a large margin. An easy way to leverage the power of CNNs, without investing time and effort into training, is to use a *pre-trained CNN* as a feature extractor. This is a CNN that is already trained on a very large dataset.\n",
    "\n",
    "### 3.2.1 Loading Pre-Trained Weights\n",
    "\n",
    "The `torchvision` library of PyTorch [contains many popular neural network architectures](https://pytorch.org/vision/stable/models.html) with the option to load pre-trained weights. Below, we import a ResNet-50 model ([He et al., 2015](https://arxiv.org/abs/1512.03385)) and load in weights that were obtained after training the model on ImageNet1K ([Deng et al., 2009](https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/document/5206848)).\n",
    "\n",
    "We also call [`eval()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval) on the model. This sets the model in *evaluation mode*, which is important because the [BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d) layers of ResNet have a different behaviour in training and evaluation mode. Similarly, calling `train()` on a model switches the model back to training mode. **From now on, make it a habbit to always call `eval()` before evaluating your model** (so, also before your validation loop) **and `train()` before training the model.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d855c-f01d-4fa5-8b0e-02d5dc182824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet50, ResNet50_Weights\n",
    "\n",
    "# Get pre-trained ResNet-50 weights\n",
    "rn50_weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "\n",
    "# Create a ResNet-50 with the pre-trained weights\n",
    "rn50 = resnet50(weights=rn50_weights)\n",
    "\n",
    "# Set model in evaluation mode\n",
    "rn50 = rn50.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29983e35",
   "metadata": {},
   "source": [
    "If you print out the ResNet-50 model architecture, you'll see that the model is much more complex than our `SimpleCNN`. Training such a big model from scratch requires large amounts of data. With the pre-trained ImageNet weights, however, we will see that our model is already very useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44962f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bb544",
   "metadata": {},
   "source": [
    "### 3.2.2 Investigating CNN Weights\n",
    "\n",
    "The layers at the beginning of the network capture basic image features, such as edges and blobs. To see this, let's visualize the network filter weights from the first convolutional layer `conv1`. This can help build up an intuition as to why the features extracted from CNNs work so well for image recognition tasks. See also ([Zeiler and Fergus, 2013](https://arxiv.org/abs/1311.2901))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeaba4f-3e8a-4557-a603-4482d18b0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the conv1 weights and detach them from autograd\n",
    "conv1_weights = torch.clone(rn50.conv1.weight).detach()\n",
    "\n",
    "# Scale the weights in range [0, 1] for visualization purposes\n",
    "conv1_weights -= conv1_weights.min()\n",
    "conv1_weights /= conv1_weights.max()\n",
    "\n",
    "# Change order of channels for visualization purposes\n",
    "conv1_weights = conv1_weights.permute(0, 3, 2, 1)\n",
    "\n",
    "# Visualize conv1 weights\n",
    "show_ims(conv1_weights, columns=8, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c7ae8",
   "metadata": {},
   "source": [
    "Notice how the first layer of the network has learned filters for capturing blob and edge features. These \"primitive\" features are then processed by deeper network layers, which combine the early features to form higher level image features. These higher level features are better suited for recognition tasks because they combine all the primitive features into a richer image representation ([Donahue et al., 2013](https://arxiv.org/abs/1310.1531))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48331e07",
   "metadata": {},
   "source": [
    "### 3.2.3 Preparing the Data\n",
    "\n",
    "For this example, we will use `Baltech101`, a balanced version of the [Caltech 101 dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.Caltech101.html). This is a classification dataset containing 101 classes with 31 images per class. By passing in `download=True`, the dataset will be downloaded if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ef3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "baltech_101 = Baltech101(\n",
    "    root='data',    # Root data folder\n",
    "    download=True,  # Download if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c6251",
   "metadata": {},
   "source": [
    "Let's have a look at some images in the dataset. The corresponding class label is shown above each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # The number of samples to show\n",
    "\n",
    "# Generate N different numbers between 0 and len(baltech_101)\n",
    "idxs = torch.randperm(len(baltech_101))[:N]\n",
    "\n",
    "ims = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the sample indices\n",
    "for i in idxs:\n",
    "    # Select the sample at index i\n",
    "    im, label_idx = baltech_101[i]\n",
    "\n",
    "    # Map the label number to a human-readable category name\n",
    "    label = baltech_101.categories[label_idx]\n",
    "\n",
    "    ims.append(im)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "show_ims(ims, title=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a25cf8",
   "metadata": {},
   "source": [
    "To make our dataset suitable for use with our pre-trained ResNet-50 model, we should **apply the same transforms as were used during pre-training**. PyTorch makes this very easy for us. The `rn50_weights` object we used to initialize the model has an attribute `transforms()` that contains the transforms that were used. Let's create a new `baltech_101` object that will apply these transforms before returning a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "baltech_101_tfm = Baltech101(\n",
    "    root='data',\n",
    "    download=True,\n",
    "    transform=rn50_weights.transforms()  # Add transforms used during pre-training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d2825",
   "metadata": {},
   "source": [
    "### 3.2.4 Obtaining Intermediate Features\n",
    "\n",
    "The ResNet-50 model outputs 1000 numbers that correspond to the 1000 classes in ImageNet. These aren't very useful for us. Instead, we want to use the model as a *feature extractor* and thus want to obtain an intemediary feature outputted by one of the model's layers.\n",
    "\n",
    "For this, we will attach [*hooks*](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) to the layers of the ResNet-50 model. These hooks are functions that will be called each time a tensor is passed through the corresponding layer and allow us to catch the output of that layer.\n",
    "\n",
    "We have created a function for you that attaches all these hooks so you don't need to worry about it. It returns a dictionary `features` that stores each layer's most recent output, using the layer's name as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the hooks to our model\n",
    "features = add_feature_hooks(rn50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090c1de",
   "metadata": {},
   "source": [
    "When we pass an image through the network now, we'll be able to inspect the features outputted by any layer of our `rn50` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f2a01-639b-48cd-b897-b2135277526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 123  # Pick a random image to pass through the model\n",
    "\n",
    "img, label = baltech_101_tfm[img_idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    rn50(img[None, ...])  # None adds a batch dimension, which is expected by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88441b4b",
   "metadata": {},
   "source": [
    "For example, let's check out the output of layer `conv1` for the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_output = features['conv1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff91d24",
   "metadata": {},
   "source": [
    "The output is a tensor containing **1 feature map with 64 channels and a height and width of 112**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93303076",
   "metadata": {},
   "source": [
    "We visualize the 64 channels of this feature map below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b632d-301a-4554-a720-4724797e0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ims(features['conv1'][0], columns=8, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90844c1",
   "metadata": {},
   "source": [
    "As you can see, each filter in the first convolutional layer indeed emphasizes certain kinds of edges and regions in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b450a31",
   "metadata": {},
   "source": [
    "### 3.2.5 Obtaining Deep Features for Classification Training\n",
    "\n",
    "The features returned by the `conv1` layer are still very close to the pixel level and, as such, don't carry a lot of semantic information. These aren't very suitable for training a classifier. Instead, we need features returned by deeper levels of the network, so called *deep features*.\n",
    "\n",
    "The `avgpool` layer, for example, is the last layer before the fully-connected layer that does the ImageNet classification. We can see that it returns a **2048-dimensional deep feature vector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['avgpool'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6b375",
   "metadata": {},
   "source": [
    "We have implemented a function `get_dataset_features()` that collects the outputs of a certain layer of a network for an entire dataset. Below, we collect the features returned by the `avgpool` layer for the entire `Baltech101` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset_features(\n",
    "    dataset=baltech_101_tfm,\n",
    "    model=rn50.to('cuda'),\n",
    "    layer_name='avgpool',\n",
    "    batch_size=16,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Flatten X from shape (N, 2048, 1, 1) to shape (N, 2048)\n",
    "X = X.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9485183a",
   "metadata": {},
   "source": [
    "As you can see, `X` contains a 2048-dimensional vector for all 3131 samples in `Baltech101`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7f15e",
   "metadata": {},
   "source": [
    "### 3.2.6 Training an SVM Classifier on Deep Features\n",
    "\n",
    "Now that we have computed a deep feature vector for each sample in the `Baltech101` dataset, we can train a classifier to classify the features into their corresponding classes. To correctly evaluate our model, we will first split `X` (the features) and `y` (the labels) into train an test splits. You can use [scikit-learn's `train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function for this. Some notes on the arguments used:\n",
    "\n",
    "- `test_size`: How much data to use for the test dataset.\n",
    "- `stratify`: Passing in `y` here will ensure that classes are equally represented in the train an test dataset.\n",
    "- `random_state`: This number sets the state of the random generator. A certain `random_state` will always produce the same split. If you set this to `None`, the splits will be different each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862823e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a17f49",
   "metadata": {},
   "source": [
    "There are many models that we could use for classifying the deep features: a fully-connected layer with softmax activation, a decision tree, a nearest neighbours classifier,... In this example, we will use a **Support Vector Machine (SVM)** classifier.\n",
    "\n",
    "Scikit-learn is a Python library containing many implementations of machine learning algorithms, among which [SVM classification](https://scikit-learn.org/stable/modules/svm.html#classification). We will use [their `SVC` class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c67bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26734f",
   "metadata": {},
   "source": [
    "To train the SVM classifier, we simply pass the features and their corresponding labels to the method `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e091f8f",
   "metadata": {},
   "source": [
    "To evaluate the trained SVM, we predict the classes of the test features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47cfc5",
   "metadata": {},
   "source": [
    "Now, we can compute the confusion matrix and the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert confusion matrix to percentages\n",
    "cm = cm / cm.sum(axis=1)\n",
    "\n",
    "# The diagonal now contains the accuracy of each class\n",
    "# Compute the mean accuracy by averaging the elements on the diagonal\n",
    "acc = cm.diagonal().mean()\n",
    "\n",
    "# Print the mean accuracy\n",
    "print(f'Mean accuracy: {acc*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc740c",
   "metadata": {},
   "source": [
    "### 3.2.7 Exercise\n",
    "\n",
    "- Use [`torch.randn()`](https://pytorch.org/docs/stable/generated/torch.randn.html) to create a random tensor that can be used as input to our ResNet-50 model. Pass the random tensor through the model and report the output shapes of all layers.\n",
    "- What happens to the image's width and height as an image passes through the model? What happens to the number of channels of the input? Explain which operations cause these changes. Show how you could manually compute the output shape of `layer4`, given the shape of its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6291f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "976b6601",
   "metadata": {},
   "source": [
    "# 4. Report\n",
    "\n",
    "Write a report of maximum 4 pages (including text and figures) to discuss your answers to the exercises in section\n",
    "\n",
    "- **1.2.2**;\n",
    "- **2.3**;\n",
    "- **3.1.5** and **3.2.7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfd8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
